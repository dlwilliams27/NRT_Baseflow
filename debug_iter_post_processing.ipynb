{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-12T21:08:18.068389Z",
     "start_time": "2026-01-12T21:08:18.044194Z"
    }
   },
   "source": [
    "def nwm_processing(nwm_path, nwm_path_list):\n",
    "    print(\"Running NWM Processing\")\n",
    "    #create list of nwm region folders\n",
    "    for folder in nwm_path.iterdir():\n",
    "        match1=re.match(r'region_(\\d+)',folder.name)\n",
    "        if match1:\n",
    "            path1= nwm_path / f'{match1[0]}'\n",
    "            nwm_path_list.append(path1)\n",
    "    #iterating through files in each region folder\n",
    "    for subfolder in nwm_path_list:\n",
    "        # issue is that these are glob objects and not iterable, need them to be pure paths\n",
    "        for file in subfolder.glob('*.nc'):\n",
    "            match = re.match(r\"NWM_gage_(\\d+)\", file.name)\n",
    "            if match:\n",
    "                gage = match[1].zfill(8)\n",
    "                ds=xr.open_dataset(file)\n",
    "                nwm = ds.to_dataframe()\n",
    "                nwm['baseflow'] = nwm['q_lateral'] + nwm['qBucket']\n",
    "                nwm.reset_index(inplace=True)\n",
    "                nwm['date'] = nwm['time'].dt.date\n",
    "                nwm_avg_base = nwm.groupby('date', sort=True)['baseflow'].mean()\n",
    "                nwm_avg_stream = nwm.groupby('date', sort=True)['streamflow'].mean()\n",
    "                nwm_tot = pd.merge(nwm_avg_base, nwm_avg_stream, left_index=True, right_index=True)\n",
    "                nwm_tot.reset_index(inplace=True)\n",
    "                nwm_tot.set_index('date', inplace=True)\n",
    "                nwm_dic[gage] = nwm_tot\n",
    "            else:\n",
    "                print(f\"No match for file {nc}.\")\n",
    "    return nwm_dic\n",
    "\n",
    "#process through each file in usgs streamflow\n",
    "def usgs_processing(usgs_path):\n",
    "    print(\"Running USGS Processing\")\n",
    "    stream_path= usgs_path / \"USGS_Streamflow_2024\"\n",
    "    for folder in stream_path.iterdir():\n",
    "        match1=re.match(r'(\\d+)',folder.name)\n",
    "        if match1:\n",
    "            path2= stream_path / f'{match1.group(1)}'\n",
    "            usgs_path_list.append(path2)\n",
    "            for subfolder in usgs_path_list:\n",
    "                for file in subfolder.glob('*.txt'):\n",
    "                    try:\n",
    "                        match2 = re.match(r\"(\\d+)_streamflow_qc\", file.name)\n",
    "                        if match2:\n",
    "                            gage2=match2.group(1)\n",
    "                            usgs = pd.read_csv(file, sep=' ', on_bad_lines='skip')\n",
    "                            if len(usgs.columns) == 7:\n",
    "                                usgs.columns = ['gage', 'year', 'month', 'day', 'NAN', 'Q', 'nAn']\n",
    "                            elif len(usgs.columns) == 8:\n",
    "                                usgs.columns = ['gage', 'year', 'month', 'day', 'NAN', 'nan', 'Q', 'nAn']\n",
    "                            elif len(usgs.columns) == 9:\n",
    "                                usgs.columns = ['gage', 'year', 'month', 'day', 'NAN', 'nan', 'NaN', 'Q', 'nAn']\n",
    "                            elif len(usgs.columns) == 10:\n",
    "                                usgs.columns = ['gage', 'year', 'month', 'day', 'NAN', 'nan', 'NaN', 'NAn', 'Q', 'nAn']\n",
    "                            usgs = usgs[['gage', 'year', 'month', 'day', 'Q']]\n",
    "                            usgs.dropna(inplace=True)\n",
    "                            usgs['day']=usgs['day'].astype(int)\n",
    "                            usgs['date'] = pd.to_datetime(usgs[['year', 'month', 'day']])\n",
    "                            usgs.set_index('date', inplace=True)\n",
    "                            usgs = usgs['Q']\n",
    "                            usgs_dic[gage2] = usgs\n",
    "                    except KeyError:\n",
    "                        print(f\"Column indexing error for file: {file}\")\n",
    "                        continue\n",
    "    return usgs_dic\n",
    "\n",
    "#process through each transformed eckhardt baseflow file\n",
    "def eck_processing(usgs_path):\n",
    "    print(\"Running Eck Processing\")\n",
    "    eck_path = usgs_path / \"Eckhardt_2024\"\n",
    "    for file in eck_path.glob('*.csv'):\n",
    "        match3 = re.match(r\"(\\d+)_streamflow_qc_processed\", file.name)\n",
    "        if match3:\n",
    "            gage=match3.group(1)\n",
    "            eck=pd.read_csv(file)\n",
    "            eck['date'] = pd.to_datetime(eck['date'])\n",
    "            eck.set_index('date', inplace=True)\n",
    "            eck_dic[gage]=eck\n",
    "    return eck_dic\n",
    "\n",
    "#merge together NWM, USGS, and Eckhardt dfs based off of gage ID key\n",
    "def merge_dicts(nwm_dic, usgs_dic, eck_dic):\n",
    "    print(\"Merging Dictionaries\")\n",
    "    complete=[]\n",
    "    common_keys= list(set(nwm_dic).intersection(usgs_dic, eck_dic))\n",
    "    print(\"Common Keys:\", common_keys)\n",
    "    for key in common_keys:\n",
    "        df1=nwm_dic[key].copy()\n",
    "        df2=usgs_dic[key].copy()\n",
    "        df3=eck_dic[key].copy()\n",
    "        df3['gage']=key\n",
    "        middle=pd.merge(df1, df2, left_index=True, right_index=True)\n",
    "        final=pd.merge(middle, df3, left_index=True, right_index=True)\n",
    "        complete.append(final)\n",
    "    complete=pd.concat(complete, ignore_index=False)\n",
    "    complete['Q_was_non_numeric'] = (\n",
    "            pd.to_numeric(complete['Q'], errors='coerce').isna()\n",
    "            & complete['Q'].notna())\n",
    "    complete['Eckhardt']=pd.to_numeric(complete['Eckhardt'], errors='coerce')\n",
    "    complete['Q']=pd.to_numeric(complete['Q'], errors='coerce')\n",
    "    complete['BFIobs']=(complete['Eckhardt'].astype(float))/(complete['Q'].astype(float)) #issue with data types here\n",
    "\n",
    "    complete['BFIsim']=complete['baseflow']/complete['streamflow']\n",
    "\n",
    "    finaloutput_path = base_path / 'Complete_inputs4stats.csv'\n",
    "    finaloutput_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    complete.to_csv(finaloutput_path, index=True)\n",
    "    return complete\n",
    "\n",
    "#creates a new column which identifies the season of the year\n",
    "def seasons(df):\n",
    "    conditions = [\n",
    "        df['month'].isin([12, 1, 2]),\n",
    "        df['month'].isin([3, 4, 5]),\n",
    "        df['month'].isin([6, 7, 8]),\n",
    "        df['month'].isin([9, 10, 11]),\n",
    "    ]\n",
    "    choices = [1, 2, 3, 4]\n",
    "    df['season'] = np.select(conditions, choices, default=np.nan)\n",
    "    return df\n",
    "\n",
    "#processes KGE, NSE, and Pearson R for all time frames and outputting\n",
    "def stats(final_df):\n",
    "    print(\"Calculating Statistics\")\n",
    "    #resets date index, and creates year, month, and season columns\n",
    "    final_df.reset_index(inplace=True)\n",
    "    final_df['year']=final_df['date'].dt.year\n",
    "    final_df['month']=final_df['date'].dt.month\n",
    "    final_df=seasons(final_df)\n",
    "\n",
    "    #empty list\n",
    "    rows=[]\n",
    "    rows2=[]\n",
    "    rows3=[]\n",
    "\n",
    "    #RMSE, KGE, and Pearson R calculation for all time\n",
    "    grouped=final_df.groupby(['gage'])\n",
    "    for gage, group in grouped:\n",
    "        rows.append({'gage': gage, 'nse_o': he.nse(group['baseflow'].to_numpy(), group['Eckhardt'].to_numpy()),\n",
    "                     'rmse_o': he.rmse(group['baseflow'],group['Eckhardt']),\n",
    "                     'kge_12_o': he.kge_2012(group['baseflow'].to_numpy(), group['Eckhardt'].to_numpy()),\n",
    "                     'pearson_o': he.pearson_r(group['baseflow'],group['Eckhardt'])}\n",
    "        )\n",
    "    overall_stats=pd.DataFrame(rows)\n",
    "    overall_path= base_path / 'Overall_stats.csv'\n",
    "    overall_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    overall_stats.to_csv(overall_path, index=True)\n",
    "\n",
    "    #RMSE, KGE, and Pearson R calculation for each year\n",
    "    grouped2=final_df.groupby(['gage', 'year'])\n",
    "    for (gage, year), group2 in grouped2:\n",
    "        rows2.append({'gage': gage, 'year': year, 'nse_o': he.nse(group2['baseflow'].to_numpy(), group2['Eckhardt'].to_numpy()),\n",
    "                     'rmse_o': he.rmse(group2['baseflow'], group2['Eckhardt']),\n",
    "                     'kge_12_o': he.kge_2012(group2['baseflow'].to_numpy(), group2['Eckhardt'].to_numpy()),\n",
    "                     'pearson_o': he.pearson_r(group2['baseflow'], group2['Eckhardt'])}\n",
    "                    )\n",
    "    year_stats=pd.DataFrame(rows2)\n",
    "    year_path = base_path / 'year_stats.csv'\n",
    "    year_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    year_stats.to_csv(year_path, index=True)\n",
    "\n",
    "    #RMSE, KGE, and Pearson R calculation for each season\n",
    "    final_df=seasons(final_df)\n",
    "    grouped3=final_df.groupby(['gage', 'season'])\n",
    "    for (gage, season), group3 in grouped3:\n",
    "        rows3.append(\n",
    "            {'gage': gage, 'season': season, 'nse_o': he.nse(group3['baseflow'].to_numpy(), group3['Eckhardt'].to_numpy()),\n",
    "             'rmse_o': he.rmse(group3['baseflow'], group3['Eckhardt']),\n",
    "             'kge_12_o': he.kge_2012(group3['baseflow'].to_numpy(), group3['Eckhardt'].to_numpy()),\n",
    "             'pearson_o': he.pearson_r(group3['baseflow'], group3['Eckhardt'])}\n",
    "            )\n",
    "    season_stats = pd.DataFrame(rows3)\n",
    "    season_path = base_path / 'seasonal_stats.csv'\n",
    "    season_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    season_stats.to_csv(season_path, index=True)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T21:08:38.947797Z",
     "start_time": "2026-01-12T21:08:37.563627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pathlib\n",
    "import HydroErr as he\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import re\n",
    "import pathlib as path\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "from collections import defaultdict\n",
    "\n",
    "#creating initial file paths\n",
    "nwm_file_dir=r'C:\\Users\\Delanie Williams\\OneDrive - The University of Alabama\\Research\\Baseflow Project\\NWM_Results'\n",
    "nwm_path= path.Path(nwm_file_dir)\n",
    "usgs_file_dir=r'C:\\Users\\Delanie Williams\\OneDrive - The University of Alabama\\Research\\Baseflow Project\\Initial_Results'\n",
    "usgs_path= path.Path(usgs_file_dir)\n",
    "base_dir=r'C:\\Users\\Delanie Williams\\OneDrive - The University of Alabama\\Research\\Baseflow Project'\n",
    "base_path= path.Path(base_dir)\n",
    "\n",
    "#initializing dictionaries and lists\n",
    "nwm_dic={}\n",
    "usgs_dic={}\n",
    "eck_dic={}\n",
    "nwm_path_list=[]\n",
    "usgs_path_list=[]\n"
   ],
   "id": "eaa67c039301e64b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T21:10:35.267375Z",
     "start_time": "2026-01-12T21:08:57.818170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nwm_dic=nwm_processing(nwm_path, nwm_path_list)\n",
    "usgs_dic=usgs_processing(usgs_path)\n",
    "eck_dic=eck_processing(usgs_path)"
   ],
   "id": "dc51ea32def52512",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running NWM Processing\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m nwm_dic=\u001B[43mnwm_processing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnwm_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnwm_path_list\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      2\u001B[39m usgs_dic=usgs_processing(usgs_path)\n\u001B[32m      3\u001B[39m eck_dic=eck_processing(usgs_path)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 21\u001B[39m, in \u001B[36mnwm_processing\u001B[39m\u001B[34m(nwm_path, nwm_path_list)\u001B[39m\n\u001B[32m     19\u001B[39m nwm.reset_index(inplace=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     20\u001B[39m nwm[\u001B[33m'\u001B[39m\u001B[33mdate\u001B[39m\u001B[33m'\u001B[39m] = nwm[\u001B[33m'\u001B[39m\u001B[33mtime\u001B[39m\u001B[33m'\u001B[39m].dt.date\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m nwm_avg_base = \u001B[43mnwm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgroupby\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdate\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msort\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mbaseflow\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     22\u001B[39m nwm_avg_stream = nwm.groupby(\u001B[33m'\u001B[39m\u001B[33mdate\u001B[39m\u001B[33m'\u001B[39m, sort=\u001B[38;5;28;01mTrue\u001B[39;00m)[\u001B[33m'\u001B[39m\u001B[33mstreamflow\u001B[39m\u001B[33m'\u001B[39m].mean()\n\u001B[32m     23\u001B[39m nwm_tot = pd.merge(nwm_avg_base, nwm_avg_stream, left_index=\u001B[38;5;28;01mTrue\u001B[39;00m, right_index=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\NRT_Baseflow\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:2459\u001B[39m, in \u001B[36mGroupBy.mean\u001B[39m\u001B[34m(self, numeric_only, engine, engine_kwargs)\u001B[39m\n\u001B[32m   2452\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._numba_agg_general(\n\u001B[32m   2453\u001B[39m         grouped_mean,\n\u001B[32m   2454\u001B[39m         executor.float_dtype_mapping,\n\u001B[32m   2455\u001B[39m         engine_kwargs,\n\u001B[32m   2456\u001B[39m         min_periods=\u001B[32m0\u001B[39m,\n\u001B[32m   2457\u001B[39m     )\n\u001B[32m   2458\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2459\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_cython_agg_general\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2460\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmean\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   2461\u001B[39m \u001B[43m        \u001B[49m\u001B[43malt\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mSeries\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnumeric_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnumeric_only\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2462\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnumeric_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnumeric_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2463\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2464\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m result.__finalize__(\u001B[38;5;28mself\u001B[39m.obj, method=\u001B[33m\"\u001B[39m\u001B[33mgroupby\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\NRT_Baseflow\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:2005\u001B[39m, in \u001B[36mGroupBy._cython_agg_general\u001B[39m\u001B[34m(self, how, alt, numeric_only, min_count, **kwargs)\u001B[39m\n\u001B[32m   2002\u001B[39m     result = \u001B[38;5;28mself\u001B[39m._agg_py_fallback(how, values, ndim=data.ndim, alt=alt)\n\u001B[32m   2003\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[32m-> \u001B[39m\u001B[32m2005\u001B[39m new_mgr = \u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgrouped_reduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43marray_func\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2006\u001B[39m res = \u001B[38;5;28mself\u001B[39m._wrap_agged_manager(new_mgr)\n\u001B[32m   2007\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m how \u001B[38;5;129;01min\u001B[39;00m [\u001B[33m\"\u001B[39m\u001B[33midxmin\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33midxmax\u001B[39m\u001B[33m\"\u001B[39m]:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\NRT_Baseflow\\Lib\\site-packages\\pandas\\core\\internals\\base.py:367\u001B[39m, in \u001B[36mSingleDataManager.grouped_reduce\u001B[39m\u001B[34m(self, func)\u001B[39m\n\u001B[32m    365\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgrouped_reduce\u001B[39m(\u001B[38;5;28mself\u001B[39m, func):\n\u001B[32m    366\u001B[39m     arr = \u001B[38;5;28mself\u001B[39m.array\n\u001B[32m--> \u001B[39m\u001B[32m367\u001B[39m     res = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    368\u001B[39m     index = default_index(\u001B[38;5;28mlen\u001B[39m(res))\n\u001B[32m    370\u001B[39m     mgr = \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m).from_array(res, index)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\NRT_Baseflow\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1980\u001B[39m, in \u001B[36mGroupBy._cython_agg_general.<locals>.array_func\u001B[39m\u001B[34m(values)\u001B[39m\n\u001B[32m   1978\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34marray_func\u001B[39m(values: ArrayLike) -> ArrayLike:\n\u001B[32m   1979\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1980\u001B[39m         result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_grouper\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_cython_operation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1981\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43maggregate\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1982\u001B[39m \u001B[43m            \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1983\u001B[39m \u001B[43m            \u001B[49m\u001B[43mhow\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1984\u001B[39m \u001B[43m            \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mndim\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1985\u001B[39m \u001B[43m            \u001B[49m\u001B[43mmin_count\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmin_count\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1986\u001B[39m \u001B[43m            \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1987\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1988\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m:\n\u001B[32m   1989\u001B[39m         \u001B[38;5;66;03m# generally if we have numeric_only=False\u001B[39;00m\n\u001B[32m   1990\u001B[39m         \u001B[38;5;66;03m# and non-applicable functions\u001B[39;00m\n\u001B[32m   1991\u001B[39m         \u001B[38;5;66;03m# try to python agg\u001B[39;00m\n\u001B[32m   1992\u001B[39m         \u001B[38;5;66;03m# TODO: shouldn't min_count matter?\u001B[39;00m\n\u001B[32m   1993\u001B[39m         \u001B[38;5;66;03m# TODO: avoid special casing SparseArray here\u001B[39;00m\n\u001B[32m   1994\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m how \u001B[38;5;129;01min\u001B[39;00m [\u001B[33m\"\u001B[39m\u001B[33many\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mall\u001B[39m\u001B[33m\"\u001B[39m] \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(values, SparseArray):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\NRT_Baseflow\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:836\u001B[39m, in \u001B[36mBaseGrouper._cython_operation\u001B[39m\u001B[34m(self, kind, values, how, axis, min_count, **kwargs)\u001B[39m\n\u001B[32m    831\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    832\u001B[39m \u001B[33;03mReturns the values of a cython operation.\u001B[39;00m\n\u001B[32m    833\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    834\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m kind \u001B[38;5;129;01min\u001B[39;00m [\u001B[33m\"\u001B[39m\u001B[33mtransform\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33maggregate\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m--> \u001B[39m\u001B[32m836\u001B[39m cy_op = WrappedCythonOp(kind=kind, how=how, has_dropped_na=\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mhas_dropped_na\u001B[49m)\n\u001B[32m    838\u001B[39m ids, _, _ = \u001B[38;5;28mself\u001B[39m.group_info\n\u001B[32m    839\u001B[39m ngroups = \u001B[38;5;28mself\u001B[39m.ngroups\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/properties.pyx:36\u001B[39m, in \u001B[36mpandas._libs.properties.CachedProperty.__get__\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\NRT_Baseflow\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:750\u001B[39m, in \u001B[36mBaseGrouper.has_dropped_na\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    744\u001B[39m \u001B[38;5;129m@final\u001B[39m\n\u001B[32m    745\u001B[39m \u001B[38;5;129m@cache_readonly\u001B[39m\n\u001B[32m    746\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhas_dropped_na\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[38;5;28mbool\u001B[39m:\n\u001B[32m    747\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    748\u001B[39m \u001B[33;03m    Whether grouper has null value(s) that are dropped.\u001B[39;00m\n\u001B[32m    749\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mbool\u001B[39m((\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgroup_info\u001B[49m[\u001B[32m0\u001B[39m] < \u001B[32m0\u001B[39m).any())\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/properties.pyx:36\u001B[39m, in \u001B[36mpandas._libs.properties.CachedProperty.__get__\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\NRT_Baseflow\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:754\u001B[39m, in \u001B[36mBaseGrouper.group_info\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    752\u001B[39m \u001B[38;5;129m@cache_readonly\u001B[39m\n\u001B[32m    753\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgroup_info\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[38;5;28mtuple\u001B[39m[npt.NDArray[np.intp], npt.NDArray[np.intp], \u001B[38;5;28mint\u001B[39m]:\n\u001B[32m--> \u001B[39m\u001B[32m754\u001B[39m     comp_ids, obs_group_ids = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_compressed_codes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    756\u001B[39m     ngroups = \u001B[38;5;28mlen\u001B[39m(obs_group_ids)\n\u001B[32m    757\u001B[39m     comp_ids = ensure_platform_int(comp_ids)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\NRT_Baseflow\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:778\u001B[39m, in \u001B[36mBaseGrouper._get_compressed_codes\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    775\u001B[39m     \u001B[38;5;66;03m# FIXME: compress_group_index's second return value is int64, not intp\u001B[39;00m\n\u001B[32m    777\u001B[39m ping = \u001B[38;5;28mself\u001B[39m.groupings[\u001B[32m0\u001B[39m]\n\u001B[32m--> \u001B[39m\u001B[32m778\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mping\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcodes\u001B[49m, np.arange(\u001B[38;5;28mlen\u001B[39m(ping._group_index), dtype=np.intp)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\NRT_Baseflow\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:691\u001B[39m, in \u001B[36mGrouping.codes\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    689\u001B[39m \u001B[38;5;129m@property\u001B[39m\n\u001B[32m    690\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcodes\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> npt.NDArray[np.signedinteger]:\n\u001B[32m--> \u001B[39m\u001B[32m691\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_codes_and_uniques\u001B[49m[\u001B[32m0\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/properties.pyx:36\u001B[39m, in \u001B[36mpandas._libs.properties.CachedProperty.__get__\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\NRT_Baseflow\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:835\u001B[39m, in \u001B[36mGrouping._codes_and_uniques\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    830\u001B[39m     uniques = \u001B[38;5;28mself\u001B[39m._uniques\n\u001B[32m    831\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    832\u001B[39m     \u001B[38;5;66;03m# GH35667, replace dropna=False with use_na_sentinel=False\u001B[39;00m\n\u001B[32m    833\u001B[39m     \u001B[38;5;66;03m# error: Incompatible types in assignment (expression has type \"Union[\u001B[39;00m\n\u001B[32m    834\u001B[39m     \u001B[38;5;66;03m# ndarray[Any, Any], Index]\", variable has type \"Categorical\")\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m835\u001B[39m     codes, uniques = \u001B[43malgorithms\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfactorize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[assignment]\u001B[39;49;00m\n\u001B[32m    836\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgrouping_vector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msort\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sort\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_na_sentinel\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dropna\u001B[49m\n\u001B[32m    837\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    838\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m codes, uniques\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\NRT_Baseflow\\Lib\\site-packages\\pandas\\core\\algorithms.py:802\u001B[39m, in \u001B[36mfactorize\u001B[39m\u001B[34m(values, sort, use_na_sentinel, size_hint)\u001B[39m\n\u001B[32m    795\u001B[39m     codes, uniques = factorize_array(\n\u001B[32m    796\u001B[39m         values,\n\u001B[32m    797\u001B[39m         use_na_sentinel=use_na_sentinel,\n\u001B[32m    798\u001B[39m         size_hint=size_hint,\n\u001B[32m    799\u001B[39m     )\n\u001B[32m    801\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m sort \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) > \u001B[32m0\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m802\u001B[39m     uniques, codes = \u001B[43msafe_sort\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    803\u001B[39m \u001B[43m        \u001B[49m\u001B[43muniques\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    804\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    805\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_na_sentinel\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_na_sentinel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    806\u001B[39m \u001B[43m        \u001B[49m\u001B[43massume_unique\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    807\u001B[39m \u001B[43m        \u001B[49m\u001B[43mverify\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    808\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    810\u001B[39m uniques = _reconstruct_data(uniques, original.dtype, original)\n\u001B[32m    812\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m codes, uniques\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\NRT_Baseflow\\Lib\\site-packages\\pandas\\core\\algorithms.py:1563\u001B[39m, in \u001B[36msafe_sort\u001B[39m\u001B[34m(values, codes, use_na_sentinel, assume_unique, verify)\u001B[39m\n\u001B[32m   1561\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1562\u001B[39m         mask = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1563\u001B[39m     new_codes = \u001B[43mtake_nd\u001B[49m\u001B[43m(\u001B[49m\u001B[43morder2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcodes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[43m=\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1564\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1565\u001B[39m     reverse_indexer = np.empty(\u001B[38;5;28mlen\u001B[39m(sorter), dtype=\u001B[38;5;28mint\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\NRT_Baseflow\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001B[39m, in \u001B[36mtake_nd\u001B[39m\u001B[34m(arr, indexer, axis, fill_value, allow_fill)\u001B[39m\n\u001B[32m    114\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\n\u001B[32m    116\u001B[39m arr = np.asarray(arr)\n\u001B[32m--> \u001B[39m\u001B[32m117\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_take_nd_ndarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_fill\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\NRT_Baseflow\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:162\u001B[39m, in \u001B[36m_take_nd_ndarray\u001B[39m\u001B[34m(arr, indexer, axis, fill_value, allow_fill)\u001B[39m\n\u001B[32m    157\u001B[39m     out = np.empty(out_shape, dtype=dtype)\n\u001B[32m    159\u001B[39m func = _get_take_nd_function(\n\u001B[32m    160\u001B[39m     arr.ndim, arr.dtype, out.dtype, axis=axis, mask_info=mask_info\n\u001B[32m    161\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m162\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    164\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m flip_order:\n\u001B[32m    165\u001B[39m     out = out.T\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
